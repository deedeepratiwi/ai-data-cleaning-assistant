# ü§ñ AI Data Cleaning Assistant

[![Tests](https://github.com/deedeepratiwi/ai-data-cleaning-assistant/workflows/Tests/badge.svg)](https://github.com/deedeepratiwi/ai-data-cleaning-assistant/actions)

## üìå Problem Description

Raw tabular data (CSV/Excel) is often messy: inconsistent column names, missing values, mixed data types, duplicates, and unclear schemas. Cleaning this data is time-consuming, error-prone, and repetitive, especially for analysts and data scientists.

**This project builds an AI-powered data cleaning assistant** that:

* Accepts CSV or Excel files via a web UI or API
* Analyzes dataset structure and common data quality issues
* Generates and applies cleaning suggestions using intelligent detection algorithms
* Returns a cleaned dataset plus a human-readable explanation of changes

### Key Features

‚úÖ **Automated Column Standardization** - Converts column names to snake_case  
‚úÖ **Non-Value Detection** - Identifies and replaces ERROR/UNKNOWN/N/A placeholders  
‚úÖ **String Normalization** - Converts values to lowercase snake_case  
‚úÖ **Smart Type Casting** - Auto-detects and casts numeric and datetime columns  
‚úÖ **Duplicate Removal** - Identifies and removes duplicate rows  
‚úÖ **Null Handling** - Fills or drops null values  
‚úÖ **Dtype Preservation** - Saves metadata for proper CSV re-reading  
‚úÖ **ID Column Protection** - Preserves identifier patterns (EMP1000, TXN_123)

### Example Transformation

**Input CSV:**
```
Transaction ID  | Payment Method | Location   | Total Spent | Transaction Date
TXN_1961373     | Credit Card    | Takeaway   | 4.0         | 2023-09-08
TXN_4271903     | Credit Card    | In-store   | ERROR       | 2023-07-19
TXN_7034554     | UNKNOWN        | UNKNOWN    | 10.0        | 2023-04-27
```

**Cleaned CSV Output:**
```
transaction_id  | payment_method | location  | total_spent | transaction_date
TXN_1961373     | credit_card    | takeaway  | 4.0         | 2023-09-08
TXN_4271903     | credit_card    | in_store  | NaN         | 2023-07-19
TXN_7034554     | NaN            | NaN       | 10.0        | 2023-04-27
```

**Applied Transformations:**
1. Column names ‚Üí snake_case
2. ERROR/UNKNOWN ‚Üí NaN
3. Values ‚Üí lowercase (credit_card, in_store, takeaway)
4. Dates ‚Üí datetime64[ns] type (in memory; use parse_dates when reading CSV)
5. Numeric columns ‚Üí proper float64 type

**Note:** CSV format stores everything as text. Use the dtype metadata endpoint and `pd.read_csv(..., parse_dates=['transaction_date'])` to restore proper data types.

The system is designed as a **full-stack AI application** demonstrating modern AI development practices, API-first design, containerization, testing, and deployment.

---

## üöÄ Quick Start

### Prerequisites

- Python 3.11+
- pip

### Local Development Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/deedeepratiwi/ai-data-cleaning-assistant.git
   cd ai-data-cleaning-assistant
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Install Playwright browsers (for e2e tests)**
   ```bash
   python -m playwright install chromium
   ```

4. **Initialize database**
   ```bash
   python scripts/init_db.py
   ```

5. **Start the server**
   ```bash
   python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
   ```

6. **Open the landing page**
   
   Navigate to [http://localhost:8000](http://localhost:8000) in your browser

---

## üß™ Testing

### Test Coverage

The project includes comprehensive test coverage:

* **36 unit tests** for transformation operations
* **Integration tests** for complete cleaning pipeline
* **E2E tests** using Playwright for UI workflows

### Run All Tests

```bash
# Run all tests including e2e
pytest tests/ -v
```

### Run Unit Tests Only

```bash
# Run transformation unit tests
pytest tests/test_transformations.py -v

# Run specific test
pytest tests/test_transformations.py::test_standardize_case -v
```

### Run Integration Tests

```bash
# Test complete pipeline with user data scenarios
pytest tests/test_new_operations_integration.py -v
```

### Run E2E Tests

```bash
# Ensure server is running on port 8000
python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 &

# Run Playwright e2e tests
pytest tests/test_e2e_playwright.py -v
```

### Test Coverage

```bash
pytest --cov=. --cov-report=html tests/
```

### Testing Strategy

1. **Unit Tests** - Test individual transformations in isolation
   - Edge cases (empty data, all nulls, single row)
   - Type conversion scenarios
   - Parameter validation

2. **Integration Tests** - Test complete workflows
   - Multi-operation pipelines
   - Column name mapping after standardization
   - Datetime detection priority

3. **E2E Tests** - Test via UI/API
   - File upload ‚Üí cleaning ‚Üí download
   - Report generation
   - Error handling

---

## üìã Usage Example

### Using the Web UI

1. Navigate to [http://localhost:8000](http://localhost:8000)
2. Click "Choose CSV File" and select a messy CSV file
3. Click "Upload & Clean Data"
4. Wait for processing to complete
5. Download the cleaned CSV or view the cleaning report

### Using the API

```bash
# 1. Upload a file
curl -X POST http://localhost:8000/jobs/upload \
  -F "file=@dirty_data.csv"
# Response: {"job_id": "abc-123", "status": "pending"}

# 2. Start profiling (triggers full pipeline)
curl -X POST http://localhost:8000/jobs/abc-123/profile

# 3. Check status
curl http://localhost:8000/jobs/abc-123
# Response: {"status": "done", ...}

# 4. Download cleaned file
curl http://localhost:8000/jobs/abc-123/download -o cleaned.csv

# 5. Get dtype metadata (for datetime columns)
curl http://localhost:8000/jobs/abc-123/download/dtypes
# Response: {"dtypes": {...}, "datetime_columns": ["transaction_date"]}

# 6. View cleaning report
curl http://localhost:8000/jobs/abc-123/report
```

### Reading Cleaned CSV with Proper Data Types

Since CSV format doesn't preserve datetime types, use the dtype metadata:

```python
import pandas as pd
import requests

# Get dtype metadata
response = requests.get(f"http://localhost:8000/jobs/{job_id}/download/dtypes")
metadata = response.json()

# Read CSV with datetime columns properly parsed
df = pd.read_csv(
    f"cleaned_{job_id}.csv",
    parse_dates=metadata['datetime_columns']
)

# Now datetime columns have datetime64[ns] dtype!
print(df.dtypes)
```

---

## üß† AI System Development (Tools, Workflow, MCP)

### Intelligent Data Quality Detection

The system uses sophisticated detection algorithms to identify data quality issues:

#### 1. **Column Name Analysis**
```python
# Detects non-snake_case column names
needs_standardization = any(
    col != _to_snake_case(col) 
    for col in df.columns
)
# "Payment Method" ‚Üí needs standardization
```

#### 2. **Non-Value Detection** (Vectorized)
```python
# Fast pattern matching for placeholder values
non_value_indicators = {
    'ERROR', 'UNKNOWN', 'N/A', 'NULL', 'NA', 
    'n/a', 'null', 'none', 'NONE', '-', '--'
}
has_non_values = df[col].isin(non_value_indicators).any()
```

#### 3. **String Standardization Detection**
```python
# Compares actual values against snake_case format
unique_values = df[col].dropna().unique()[:100]
needs_standardization = any(
    str(v) != _to_snake_case(str(v)) 
    for v in unique_values
)
# "Active", "Bob" ‚Üí needs standardization
```

#### 4. **Numeric Type Detection**
```python
# Tests conversion success rate
numeric_count = pd.to_numeric(df[col], errors='coerce').notna().sum()
numeric_ratio = numeric_count / len(df[col])
is_numeric = numeric_ratio >= 0.5  # 50% threshold
# "123", "456.78", "ERROR" ‚Üí 67% success ‚Üí cast to numeric
```

#### 5. **DateTime Detection** (Two-Phase)
```python
# Phase 1: Name pattern matching
is_likely_date = (
    'date' in col_lower or 'time' in col_lower or
    col_lower.endswith('_at') or col_lower.endswith('_on') or
    col_lower in ['created', 'updated', 'modified', 'deleted']
)

# Phase 2: Conversion testing
datetime_count = pd.to_datetime(df[col], errors='coerce').notna().sum()
datetime_ratio = datetime_count / len(df[col])
is_datetime = is_likely_date and datetime_ratio >= 0.8  # 80% threshold
```

#### 6. **Duplicate Detection**
```python
# Fast duplicate row detection
has_duplicates = df.duplicated().any()
```

### Suggestion Ordering Strategy

Suggestions are generated in a specific order to avoid conflicts:

1. **Column standardization first** - So subsequent operations use correct names
2. **Non-value replacement** - Before type casting (ERROR ‚Üí NaN)
3. **String standardization** - But skip datetime columns
4. **Numeric casting** - Convert string numbers
5. **Datetime casting** - After string standardization is skipped
6. **Duplicate removal** - After all transformations
7. **Null handling** - Last step

### AI-Assisted Development

AI tools were used throughout development to:

* Generate and refactor backend and frontend code
* Design prompts for data quality analysis and cleaning suggestions
* Draft OpenAPI specifications and test cases
* Improve documentation and developer ergonomics

### Prompt & Workflow Design

The LLM is used in a **tool-augmented workflow**:

1. Dataset schema and statistics are extracted programmatically
2. Structured context is sent to the LLM (column types, missing values, samples)
3. The LLM proposes cleaning actions in a structured JSON format
4. The backend validates and executes these actions deterministically

### MCP (Model Context Protocol)

This project uses MCP concepts by:

* Defining **clear tool boundaries** between LLM reasoning and execution
* Treating the LLM as a planner, not an executor
* Using structured inputs/outputs (JSON schemas) as the contract between model and system

An example MCP-style tool interface is documented in `AGENTS.md`.

---

## üèóÔ∏è Technologies & System Architecture

### Tech Stack

| Layer            | Technology              | Role                                    |
| ---------------- | ----------------------- | --------------------------------------- |
| Frontend         | React + TypeScript      | File upload, preview, results UI        |
| Backend          | FastAPI (Python)        | API, LLM orchestration, data processing |
| AI               | OpenAI-compatible LLM   | Data issue detection & suggestions      |
| Data             | Pandas                  | Data profiling & cleaning               |
| Database         | SQLite / Postgres       | Job metadata & history                  |
| API Spec         | OpenAPI                 | Frontend‚Äìbackend contract               |
| Containerization | Docker & docker-compose | Local & production runs                 |
| CI/CD            | GitHub Actions          | Tests & deployment                      |

### Architecture Overview

```
Frontend (React)
   ‚Üì OpenAPI
Backend (FastAPI)
   ‚Üì
Data Profiler ‚Üí Suggestion Service ‚Üí Transformation Registry
   ‚Üì JSON Actions
Deterministic Cleaning Engine
   ‚Üì
Database + Cleaned Outputs + Metadata
```

### Data Cleaning Pipeline

The system implements a comprehensive 8-step cleaning pipeline:

1. **Column Name Standardization** (`standardize_column_names`)
   - Converts column names to lowercase snake_case
   - Example: "Payment Method" ‚Üí "payment_method"
   - Removes trailing/leading spaces

2. **Non-Value Replacement** (`replace_non_values`)
   - Detects placeholder values: ERROR, UNKNOWN, N/A, NULL, etc.
   - Replaces with NaN for proper null handling
   - Example: "ERROR" ‚Üí NaN

3. **String Case Standardization** (`standardize_case`)
   - Converts string values to lowercase snake_case
   - Example: "Credit Card" ‚Üí "credit_card", "Active" ‚Üí "active"
   - **Skips ID columns** (ending with `_id` or starting with `id_`)
   - **Skips datetime columns** to preserve date formats
   - Removes trailing/leading spaces

4. **Numeric Type Casting** (`auto_cast_type`)
   - Detects numeric values stored as strings
   - Converts to Int64 or float64
   - Uses 50% threshold for mixed columns
   - Example: "123.45" ‚Üí 123.45 (float64)

5. **DateTime Type Casting** (`auto_cast_datetime`)
   - Detects date/datetime columns by name patterns
   - Patterns: 'date', 'time', '_at', '_on', 'created', 'updated', etc.
   - Converts to datetime64[ns] type
   - Uses 80% threshold for mixed columns
   - Supports ISO, slash, dash formats
   - Example: "2023-09-08" ‚Üí Timestamp('2023-09-08')

6. **Duplicate Removal** (`remove_duplicates`)
   - Automatically detects duplicate rows
   - Keeps first occurrence by default
   - Configurable: `keep='first'`, `keep='last'`, or `keep=False`

7. **Null Value Handling** (`fill_nulls` / `drop_null_rows`)
   - Fills nulls with specified values
   - Or drops rows with null values
   - Only suggested after previous operations

8. **Metadata Preservation**
   - Saves dtype information in `{job_id}_dtypes.json`
   - Includes datetime column list for CSV re-reading
   - Accessible via `/jobs/{id}/download/dtypes` endpoint

---

## üé® Frontend Implementation

* Built with **React + TypeScript**
* Centralized API client based on OpenAPI spec
* Clear separation between UI components and data services
* Supports:

  * File upload
  * Data preview
  * Cleaning summary display

### Frontend Tests

* Unit tests for core logic using Vitest
* Run tests with:

```bash
npm test
```

---

## üìú API Contract (OpenAPI)

* OpenAPI spec defined in `openapi.yaml`
* Used as the **single source of truth** for:

  * Backend endpoint implementation
  * Frontend API client generation

### Key Endpoints

#### Job Management
* `POST /jobs/upload` ‚Äì Upload CSV file, returns job_id
* `GET /jobs/{id}` ‚Äì Get job status and metadata
* `POST /jobs/{id}/profile` ‚Äì Start profiling and cleaning pipeline

#### Data Operations
* `POST /jobs/{id}/profile` ‚Äì Analyze dataset and generate suggestions
* `POST /jobs/{id}/apply` ‚Äì Apply cleaning suggestions
* `GET /jobs/{id}/download` ‚Äì Download cleaned CSV file
* `GET /jobs/{id}/download/dtypes` ‚Äì Get dtype metadata (NEW)
* `GET /jobs/{id}/report` ‚Äì Get human-readable cleaning report

#### Available Transformations

| Operation | Description | Parameters |
|-----------|-------------|------------|
| `standardize_column_names` | Convert column names to snake_case | None |
| `replace_non_values` | Replace ERROR/UNKNOWN with NaN | `column` |
| `standardize_case` | Convert values to lowercase snake_case | `column` |
| `auto_cast_type` | Cast numeric strings to Int64/float | `column` |
| `auto_cast_datetime` | Cast date strings to datetime64[ns] | `column` |
| `remove_duplicates` | Remove duplicate rows | `subset`, `keep` |
| `fill_nulls` | Fill null values | `column`, `value` |
| `drop_null_rows` | Drop rows with nulls | `column` |
| `cast_type` | Manual type casting | `column`, `dtype` |
| `drop_column` | Remove column | `column` |

---

## ‚öôÔ∏è Backend Implementation

* Built with **FastAPI** following OpenAPI contract
* Layered structure:

  * **API Routers** - HTTP endpoint handlers
  * **Services** - Business logic layer
  * **Transformations** - Data cleaning operations
  * **Core** - Database and utilities

### Service Layer Architecture

#### 1. **Job Service** (`job_service.py`)
   - Manages job lifecycle (create, retrieve, update status)
   - Handles file upload and storage
   - UUID-based job IDs for security

#### 2. **Profiling Service** (`profiling_service.py`)
   - Analyzes dataset structure and statistics
   - Detects column types, null counts, unique values
   - Generates dataset summary for suggestion service

#### 3. **Suggestion Service** (`suggestion_service.py`)
   - **Intelligent detection** of data quality issues
   - Generates transformation suggestions in priority order
   - Key detection algorithms:
     * **Column name issues**: Checks for non-snake_case names
     * **Non-values**: Vectorized detection of ERROR/UNKNOWN/N/A patterns
     * **String standardization**: Compares values to snake_case format
     * **Numeric detection**: Tests conversion success rate (50% threshold)
     * **Datetime detection**: Name patterns + conversion testing (80% threshold)
     * **Duplicate detection**: Checks for duplicate rows
   - Handles column name mapping after standardization

#### 4. **Transform Service** (`transform_service.py`)
   - Executes transformation operations
   - Uses transformation registry for operation lookup
   - Validates parameters before execution

#### 5. **Apply Service** (`apply_service.py`)
   - Orchestrates the complete cleaning pipeline
   - Applies suggestions in correct order
   - Saves cleaned data and dtype metadata
   - Generates human-readable reports

#### 6. **Report Service** (`report_service.py`)
   - Creates cleaning summary reports
   - Documents applied transformations
   - Shows before/after statistics

### Transformation Registry

All cleaning operations are registered in `transformations/registry.py`:

```python
TRANSFORMATION_REGISTRY = {
    "standardize_column_names": standardize_column_names,
    "replace_non_values": replace_non_values,
    "standardize_case": standardize_case,
    "auto_cast_type": auto_cast_type,
    "auto_cast_datetime": auto_cast_datetime,
    "remove_duplicates": remove_duplicates,
    "fill_nulls": fill_nulls,
    "drop_null_rows": drop_null_rows,
    "cast_type": cast_type,
    "drop_column": drop_column,
}
```

Each operation:
- Accepts a DataFrame and parameters
- Returns the transformed DataFrame
- Handles edge cases (empty data, invalid params)
- Includes comprehensive docstrings

### Backend Tests

* Unit tests for cleaning logic
* API tests using FastAPI TestClient
* Integration tests for complete pipeline

```bash
pytest tests/ -v
```

---

## üóÑÔ∏è Database Integration

* Uses **SQLite** for local development
* Supports **Postgres** for production
* Environment-based configuration via `.env`
* Stores:

  * Job metadata
  * Cleaning actions
  * Execution status

---

## üê≥ Containerization

The entire system runs via Docker.

```bash
docker-compose up --build
```

Services included:

* frontend
* backend
* database

---

## üî¨ Integration Testing

* Integration tests are separated under `tests/integration/`
* Cover:

  * API ‚Üí database interactions
  * End-to-end cleaning workflow

```bash
pytest tests/integration
```

---

## üöÄ Deployment

* Deployed to cloud infrastructure (example: Render / Fly.io / GCP)
* Live demo URL provided in repository description
* Deployment configuration included in `deploy/`

---

## üîÅ CI/CD Pipeline

* GitHub Actions workflow:

  * Runs backend & frontend tests on every push
  * Builds Docker images
  * Deploys on main branch if tests pass

---

## üîí Security & File Management

### Secure File Handling

- Uploaded files are stored with UUID-based names to prevent path traversal attacks
- File types are validated (CSV only)
- Files are stored in a controlled `data/` directory
- No direct user input is used in file paths

### Automatic Cleanup

Clean up old files manually:

```bash
python -m core.cleanup
```

Or programmatically:

```python
from core.cleanup import cleanup_old_files, cleanup_job_files

# Remove files older than 7 days
stats = cleanup_old_files(days_old=7)

# Remove all files for a specific job
cleanup_job_files(job_id="abc-123")
```

**Recommended:** Set up a cron job or scheduled task to run cleanup weekly.

---

## ‚ôªÔ∏è Reproducibility

### Local Setup

```bash
git clone <repo>
cd ai-data-cleaning-assistant
pip install -r requirements.txt
python scripts/init_db.py
python -m uvicorn api.main:app --host 0.0.0.0 --port 8000
```

### Run Tests

```bash
pytest tests/ -v
```

### Environment Variables

Create a `.env` file for configuration (optional):

```env
MCP_URL=http://mcp:9000
DATABASE_URL=sqlite:///./data.db
```

---

## ‚úÖ Evaluation Criteria Mapping

This README explicitly documents:

* Problem definition & system goals
* AI-assisted development & MCP concepts
* Full-stack architecture & technologies
* API-first backend design
* Frontend, backend, database, containerization
* Testing, CI/CD, deployment, and reproducibility

Designed to score **maximum points** against the AI Dev Tools Zoomcamp rubric.

---

## üìÑ License

MIT
